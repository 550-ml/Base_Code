{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "################################\n",
    "###  GAT LAYER DEFINITION    ###\n",
    "################################\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Layer (GAT) as described in the paper `\"Graph Attention Networks\" <https://arxiv.org/pdf/1710.10903.pdf>`.\n",
    "\n",
    "        This operation can be mathematically described as:\n",
    "\n",
    "            e_ij = a(W h_i, W h_j)\n",
    "            α_ij = softmax_j(e_ij) = exp(e_ij) / Σ_k(exp(e_ik))\n",
    "            h_i' = σ(Σ_j(α_ij W h_j))\n",
    "\n",
    "            where h_i and h_j are the feature vectors of nodes i and j respectively, W is a learnable weight matrix,\n",
    "            a is an attention mechanism that computes the attention coefficients e_ij, and σ is an activation function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        n_heads: int,\n",
    "        concat: bool = False,\n",
    "        dropout: float = 0.4,\n",
    "        leaky_relu_slope: float = 0.2,\n",
    "    ):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads  # Number of attention heads\n",
    "        self.concat = concat  # wether to concatenate the final attention heads\n",
    "        self.dropout = dropout  # Dropout rate\n",
    "\n",
    "        if concat:  # concatenating the attention heads\n",
    "            self.out_features = out_features  # Number of output features per node\n",
    "            assert out_features % n_heads == 0  # Ensure that out_features is a multiple of n_heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:  # averaging output over the attention heads (Used in the main paper)\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        #  A shared linear transformation, parametrized by a weight matrix W is applied to every node\n",
    "        #  Initialize the weight matrix W\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, self.n_hidden * n_heads)))\n",
    "\n",
    "        # Initialize the attention weights a\n",
    "        self.a = nn.Parameter(torch.empty(size=(n_heads, 2 * self.n_hidden, 1)))\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(leaky_relu_slope)  # LeakyReLU activation function\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax activation function to the attention coefficients\n",
    "\n",
    "        self.reset_parameters()  # Reset the parameters\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Reinitialize learnable parameters.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        nn.init.xavier_normal_(self.a)\n",
    "\n",
    "    def _get_attention_scores(self, h_transformed: torch.Tensor):\n",
    "        \"\"\"calculates the attention scores e_ij for all pairs of nodes (i, j) in the graph\n",
    "        in vectorized parallel form. for each pair of source and target nodes (i, j),\n",
    "        the attention score e_ij is computed as follows:\n",
    "\n",
    "            e_ij = LeakyReLU(a^T [Wh_i || Wh_j])\n",
    "\n",
    "            where || denotes the concatenation operation, and a and W are the learnable parameters.\n",
    "\n",
    "        Args:\n",
    "            h_transformed (torch.Tensor): Transformed feature matrix with shape (n_nodes, n_heads, n_hidden),\n",
    "                where n_nodes is the number of nodes and out_features is the number of output features per node.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Attention score matrix with shape (n_heads, n_nodes, n_nodes), where n_nodes is the number of nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        source_scores = torch.matmul(h_transformed, self.a[:, : self.n_hidden, :])\n",
    "        target_scores = torch.matmul(h_transformed, self.a[:, self.n_hidden :, :])\n",
    "\n",
    "        # broadcast add\n",
    "        # (n_heads, n_nodes, 1) + (n_heads, 1, n_nodes) = (n_heads, n_nodes, n_nodes)\n",
    "        e = source_scores + target_scores.mT\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs a graph attention layer operation.\n",
    "\n",
    "        Args:\n",
    "            h (torch.Tensor): Input tensor representing node features.\n",
    "            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after the graph convolution operation.\n",
    "        \"\"\"\n",
    "        n_nodes = h.shape[0]\n",
    "\n",
    "        # Apply linear transformation to node feature -> W h\n",
    "        # output shape (n_nodes, n_hidden * n_heads)\n",
    "        h_transformed = torch.mm(h, self.W)\n",
    "        h_transformed = F.dropout(h_transformed, self.dropout, training=self.training)\n",
    "\n",
    "        # splitting the heads by reshaping the tensor and putting heads dim first\n",
    "        # output shape (n_heads, n_nodes, n_hidden)\n",
    "        h_transformed = h_transformed.view(n_nodes, self.n_heads, self.n_hidden).permute(1, 0, 2)\n",
    "\n",
    "        # getting the attention scores\n",
    "        # output shape (n_heads, n_nodes, n_nodes)\n",
    "        e = self._get_attention_scores(h_transformed)\n",
    "\n",
    "        # Set the attention score for non-existent edges to -9e15 (MASKING NON-EXISTENT EDGES)\n",
    "        connectivity_mask = -9e16 * torch.ones_like(e)\n",
    "        e = torch.where(adj_mat > 0, e, connectivity_mask)  # masked attention scores\n",
    "\n",
    "        # attention coefficients are computed as a softmax over the rows\n",
    "        # for each column j in the attention score matrix e\n",
    "        attention = F.softmax(e, dim=-1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        # final node embeddings are computed as a weighted average of the features of its neighbors\n",
    "        h_prime = torch.matmul(attention, h_transformed)\n",
    "\n",
    "        # concatenating/averaging the attention heads\n",
    "        # output shape (n_nodes, out_features)\n",
    "        if self.concat:\n",
    "            h_prime = h_prime.permute(1, 0, 2).contiguous().view(n_nodes, self.out_features)\n",
    "        else:\n",
    "            h_prime = h_prime.mean(dim=0)\n",
    "\n",
    "        return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    图注意力层\n",
    "\n",
    "    注意力分数：e_ij = a(W h_i, W h_j)\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        n_heads: int,\n",
    "        concat: bool = False,\n",
    "        dropout: float = 0.4,\n",
    "        leaky_relu_slope: float = 0.2,\n",
    "    ):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "\n",
    "        if concat:\n",
    "            self.out_features = out_features\n",
    "            assert out_features % n_heads == 0\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, self.n_hidden * n_heads)))\n",
    "        self.a = nn.Parameter(torch.empty(size=(n_heads, 2 * self.n_hidden, 1)))\n",
    "        self.leakyrelu = nn.LeakyReLU(leaky_relu_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        nn.init.xavier_normal_(self.a)\n",
    "\n",
    "    def _get_attention_scores(self, h_transformed: torch.Tensor):\n",
    "        # 核心点是a记录了注意力得分，a[head, 2 * n_hidden, 1]表示第一个头中，把向量映射成注意力得分\n",
    "        source_scores = torch.matmul(\n",
    "            h_transformed, self.a[:, : self.n_hidden, :]\n",
    "        )  # (n_heads, n_nodes, 1) 原节点注意力贡献\n",
    "        target_scores = torch.matmul(\n",
    "            h_transformed, self.a[:, self.n_hidden :, :]\n",
    "        )  # (n_heads, n_nodes, 1) 目标节点注意力贡献\n",
    "        e = source_scores + target_scores.permute(0, 2, 1)  # 广播相加\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        n_nodes = h.shape[0]\n",
    "        h_transformed = torch.mm(h, self.W)\n",
    "        h_transformed = F.dropout(h_transformed, self.dropout, training=self.training)\n",
    "        h_transformed = h_transformed.view(n_nodes, self.n_heads, self.n_hidden).permute(\n",
    "            1, 0, 2\n",
    "        )  # (n_heads, n_nodes, n_hidden)\n",
    "        e = self._get_attention_scores(h_transformed)  # (n_heads, n_nodes, n_nodes)\n",
    "        connectivity_mask = -9e15 * torch.ones_like(e)\n",
    "        e = torch.where(adj_mat > 0, e, connectivity_mask)\n",
    "        attention = F.softmax(e, dim=-1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h_transformed)\n",
    "        if self.concat:\n",
    "            h_prime = h_prime.permute(1, 0, 2).contiguous().view(n_nodes, self.out_features)\n",
    "        else:\n",
    "            h_prime = h_prime.mean(dim=0)\n",
    "        return h_prime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
